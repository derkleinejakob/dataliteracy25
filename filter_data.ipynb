{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# from scripts import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/parlaw/speech_output.csv\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "df[\"year\"] = df.apply(lambda s: int(s[\"date\"][:4]), axis=1)\n",
    "df[\"uq_agenda\"] = df[\"agenda\"]+df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29763a5",
   "metadata": {},
   "source": [
    "## Use LDA to find clusters of speeches on the same topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19dddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use speeches where speaker is associated with a party\n",
    "df_party_members = df[~(df[\"party\"] == \"-\")]\n",
    "# for now: only use speeches which were given in english or machine translation availible\n",
    "df_party_members = df_party_members[df_party_members[\"translatedText\"].notna()]\n",
    "df_party_members = df_party_members.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = df_party_members[\"translatedText\"].dropna()\n",
    "# documents = documents[documents.map(len) > 50]\n",
    "# model = lda.process_texts(documents, custom_stopwords=[], num_topics=30, n_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d853b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# for comparison: filter agenda items with keywords \n",
    "keywords = [\"(M|m)igration\", \"(M|m)igrant\", \"(R|r)efugee\", \"(A|a)sylum\", \"(T|t)hird(-)?country national\"]\n",
    "indices_keyword_identified_speeches = df_party_members.index[df_party_members[\"uq_agenda\"].str.contains(\"|\".join(keywords))].tolist()\n",
    "\n",
    "# in a seperate step had already preprocessed (tokenized, lemmatized, ...) data\n",
    "preprocessed_data = json.load(open(\"lda/preprocessed_texts.json\"))\n",
    "# TODO: remove unneccessary tokens like \"european\", \"union\", \"mr\", \"president\" ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(lda_model, dictionary, compute_coherence=True): \n",
    "    if compute_coherence: \n",
    "        print(\"Computing coherence\")\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lda_model, \n",
    "            texts=preprocessed_data, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'  # most common coherence measure\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        print(\"Coherence:\", coherence_score)\n",
    "    else: \n",
    "        coherence_score = None \n",
    "        \n",
    "    # compute which topics are related to migration \n",
    "    migration_topic_indices = lda_model.get_term_topics(lda_model.id2word.token2id[\"migration\"], minimum_probability=0)\n",
    "    print(\"Topics related to 'migration':\", len(migration_topic_indices))\n",
    "    \n",
    "    # check if any migration-related topic has probability > 0.05 (arbitrary threshold)\n",
    "    threshold = 0.05\n",
    "    high_topics = [(tid, prob) for tid, prob in migration_topic_indices if prob > threshold]\n",
    "    if len(high_topics) > 0:\n",
    "        print(f\"Topics with probability > {threshold}: {high_topics}\")\n",
    "    else:\n",
    "        print(f\"No migration-related topic has probability > {threshold}\")\n",
    "\n",
    "    # for each speech with migration-keyworded agenda, get most probable topic\n",
    "    topic_counts = Counter()\n",
    "    topic_probabilities = []\n",
    "    missmatches = 0 \n",
    "    for idx in indices_keyword_identified_speeches:\n",
    "        # if idx >= len(preprocessed_data):\n",
    "        #     break\n",
    "        bow = dictionary.doc2bow(preprocessed_data[idx])\n",
    "        topic_distribution = lda_model.get_document_topics(bow)\n",
    "        most_probable_topic = max(topic_distribution, key=lambda x: x[1])\n",
    "        topic_counts[most_probable_topic[0]] += 1\n",
    "        topic_probabilities.append(most_probable_topic[1])\n",
    "\n",
    "        if most_probable_topic[0] not in dict(migration_topic_indices):\n",
    "            missmatches += 1 # count how often the most probable topic is not a migration-related topic\n",
    "\n",
    "    print(\"Most common topics for migration-related speeches:\", topic_counts.most_common(len(migration_topic_indices)))\n",
    "    print(\"Average probability of most probable topic for migration-related speeches:\", sum(topic_probabilities) / len(topic_probabilities))\n",
    "    print(\"missmatches (most probable topic not migration-related):\", missmatches, f\"{missmatches / len(indices_keyword_identified_speeches):.2%}\")\n",
    "\n",
    "    return len(high_topics), coherence_score, missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c96126",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed_data) == len(df_party_members)\n",
    "\n",
    "# TODO: WHY NOT?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating dictionary\")\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "print(\"filtering dictionary\")\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,     # Keep tokens appearing in at least 10 docs\n",
    "    no_above=0.4,    # Remove tokens appearing in more than 40% of docs\n",
    "    keep_n=100000    # Keep only the top 100k words by frequency\n",
    ")\n",
    "corpus = [dictionary.doc2bow(l) for l in tqdm(preprocessed_data, \"Preparing corpus\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, n_topics, k_words=5):\n",
    "    for idx, topic in model.show_topics(formatted=False, num_topics=n_topics):\n",
    "        label = \", \".join([word for word, prob in topic[:k_words]])\n",
    "        print(f\"Topic {idx + 1}: {label}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e226f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topics(lda_model, corpus):\n",
    "    # choose the LDA model to use (change if you want a different one)\n",
    "    topics = []\n",
    "    for bow in tqdm(corpus, desc=\"Assigning most probable topic to each doc\"):\n",
    "        docs_topics = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "        topics.append(docs_topics)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_topic_assignments(corpus_topics, topic_id, prob_threshold=0.0):\n",
    "    total = 0\n",
    "    most_prob_count = 0\n",
    "    at_least_prob_count = 0\n",
    "\n",
    "    most_doc_indices = [] \n",
    "    most_probabilities = []\n",
    "    at_least_probabilites = []\n",
    "    for i, doc_topics in tqdm(enumerate(corpus_topics), desc=\"Counting topic assignments\"):\n",
    "        # skip empty entries\n",
    "        if not doc_topics:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        most_topic, most_prob = max(doc_topics, key=lambda x: x[1])\n",
    "        if most_topic == topic_id:\n",
    "            most_prob_count += 1\n",
    "            most_probabilities.append(most_prob)\n",
    "            most_doc_indices.append(i)\n",
    "\n",
    "        # probability of the target topic (0.0 if absent)\n",
    "        prob_map = dict(doc_topics)\n",
    "        prob = prob_map.get(topic_id, 0.0)\n",
    "        if prob >= prob_threshold:\n",
    "            at_least_prob_count += 1\n",
    "            at_least_probabilites.append(prob)\n",
    "\n",
    "    if total == 0:\n",
    "        return ({\n",
    "            \"total_docs\": 0,\n",
    "            \"most_prob_count\": 0,\n",
    "            \"at_least_prob_count\": 0,\n",
    "            \"most_avg_prob\": 0.0, \n",
    "            \"at_least_avg_prob\": 0.0,\n",
    "        }, [])\n",
    "\n",
    "    return ({\n",
    "        \"total_docs\": total,\n",
    "        \"most_prob_count\": most_prob_count,\n",
    "        \"at_least_prob_count\": at_least_prob_count, \n",
    "        \"most_avg_prob\": sum(most_probabilities) / most_prob_count,\n",
    "        \"at_least_avg_prob\": sum(at_least_probabilites) / at_least_prob_count,  \n",
    "    }, most_doc_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# all_n_topics = [60, 80, 90, 100, 120]\n",
    "\n",
    "# all_n_topics = [80]\n",
    "\n",
    "# for n_topics in all_n_topics: \n",
    "n_topics = 80\n",
    "model = LdaModel.load(f\"lda/{n_topics}_topics/model.model\")\n",
    "evaluate_model(model, dictionary, compute_coherence=False)\n",
    "print_topics(model, n_topics)\n",
    "corpus_topics = assign_topics(model, corpus)\n",
    "\n",
    "migration_topic_index = 26\n",
    "print(\"counting occurance of topic\", migration_topic_index)\n",
    "print(list(map(lambda x: x[0], model.show_topic(topicid=26, topn=10))))\n",
    "\n",
    "counts, assigned_subset = count_topic_assignments(corpus_topics, topic_id=26, prob_threshold=0.3)\n",
    "print(counts)\n",
    "\n",
    "df_assigned_topic_most_probable = df_party_members.iloc[assigned_subset]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "n_per_year = df_assigned_topic_most_probable.groupby([\"year\", \"party\"]).size().unstack()\n",
    "\n",
    "ax = n_per_year.plot(kind=\"bar\", stacked=False, figsize=(40, 5))\n",
    "# shortened_labels = [l[:50]+\"...\" if len(l) > 50 else l for l in [lb.get_text() for lb in ax.get_xticklabels()]]\n",
    "# _ = ax.set_xticklabels(shortened_labels)\n",
    "# ax.set_ybound(upper=400)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_agenda = df_assigned_topic_most_probable.groupby([\"uq_agenda\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_per_agenda[(n_per_agenda > 10)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_assigned_topic_most_probable[df_assigned_topic_most_probable[\"uq_agenda\"].isin(n_per_agenda[(n_per_agenda > 20)].index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: look at Topic 68: border, cooperation, crime, state, member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4946a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mWeitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "count_topic_assignments(corpus_topics_m80, topic_id=26, prob_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b397f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topic_values = [90, 100, 120]\n",
    "n_workers = 8\n",
    "\n",
    "import os \n",
    "for n_topics in n_topic_values: \n",
    "    os.makedirs(f\"lda/{n_topics}_topics\", exist_ok=True)\n",
    "    out_path = f\"lda/{n_topics}_topics/model.model\"\n",
    "    num_topics = n_topics\n",
    "    n_passes = 5\n",
    "    workers = n_workers\n",
    "\n",
    "    print(\"Fitting model with\", num_topics, \"topics and\", n_passes, \"passes\")\n",
    "    lda_model = LdaMulticore(corpus = corpus, id2word=dictionary, num_topics = num_topics, passes = n_passes, workers=workers)\n",
    "    lda_model.save(out_path)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(lda_model, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f401f18",
   "metadata": {},
   "source": [
    "## Use Keyword search to find relevant agendas / speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"(M|m)igration\", \"(M|m)igrant\", \"(R|r)efugee\", \"(A|a)sylum\", \"(T|t)hird(-)?country national\"]#, \"(F|f)rontex\"]\n",
    "\n",
    "# TODO: potentially find relevant keywords using word2vec over text and look for words similar to migration? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_keywords(df, column=\"agenda\", keywords=keywords, min_contributions_per_agenda_item=10): \n",
    "    relevant_indices = df[column].str.contains(\"|\".join(keywords))\n",
    "    agenda_items_vc = df[relevant_indices][\"uq_agenda\"].value_counts()\n",
    "    \n",
    "    # TODO: do this filtering in the beginning because why not\n",
    "    n_agenda_items_before = len(agenda_items_vc)\n",
    "    relevant_agendas = agenda_items_vc[agenda_items_vc > min_contributions_per_agenda_item]\n",
    "    n_agenda_items_after = len(relevant_agendas)\n",
    "    print(f\"filtered {n_agenda_items_before-n_agenda_items_after} agenda items with < {min_contributions_per_agenda_item} speeches\")\n",
    "    \n",
    "    relevant_indices = relevant_indices & (df[\"uq_agenda\"].isin(relevant_agendas.index))\n",
    "    \n",
    "    print(f\"n speeches: {len(df[relevant_indices])}\")\n",
    "    print(f\"n agendas: {n_agenda_items_after}\")\n",
    "    \n",
    "    return df[relevant_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agenda items per year; speech per year; \n",
    "# party per agenda item\n",
    "# contribution per party per year\n",
    "# normalize by original \n",
    "df_filtered = filter_for_keywords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_year_and_party = df_filtered.groupby([\"year\"]).size()#.unstack()\n",
    "\n",
    "ax = n_per_year_and_party.plot(kind=\"bar\", stacked=False, figsize=(40, 5))\n",
    "shortened_labels = [l[:50]+\"...\" if len(l) > 50 else l for l in [lb.get_text() for lb in ax.get_xticklabels()]]\n",
    "_ = ax.set_xticklabels(shortened_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fda56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df_filtered[\"uq_agenda\"].value_counts()\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_text = filter_for_keywords(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_filtered_text[\"uq_agenda\"].value_counts()\n",
    "print(n.min(), n.mean(), n.max())\n",
    "\n",
    "#df_filtered[\"uq_agenda\"]\n",
    "print(len(n))\n",
    "# agendas that with more than 4 speeches with relevant keywords, that are not in the agendas that were discovered with keywords in title\n",
    "new_relevant_agendas = n[(n>4) & (~n.index.isin(df_filtered[\"uq_agenda\"]))]\n",
    "new_relevant_agendas[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_agenda(search_term, n_speeches=None): \n",
    "    all_agendas = df[df[\"uq_agenda\"].str.contains(search_term)][\"uq_agenda\"].unique()\n",
    "    for agenda in all_agendas: \n",
    "        speeches = df[df[\"uq_agenda\"] == agenda].sort_values(by=\"speechnumber\", ascending=True)\n",
    "        print(\"Agenda:\", (agenda[:30]+\"...\" if len(agenda) > 30 else agenda))\n",
    "        print(\"Nr of speeches:\", len(speeches))\n",
    "        print(\"\")\n",
    "        \n",
    "        if n_speeches:\n",
    "            speeches = speeches[:n_speeches]\n",
    "        for _, r in speeches.iterrows(): \n",
    "            print(f'({r[\"speechnumber\"]}) {r[\"speaker\"]} ({r['party']}): {r['text'] if not r['translatedText'] else r[\"translatedText\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_agenda(\"6.4. Situation of fundamental rights in the European Union in 2015\", 10)\n",
    "# print_agenda(\"7.7. The situation of women refugees and asylum seekers in the EU\", 10)\n",
    "print_agenda(\"European Border and Coast Guard\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f666c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
