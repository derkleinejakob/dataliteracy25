{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d78604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from ipywidgets import Button, HTML, Output\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45233e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"../../data/lda/df_30topics10passes.csv\"\n",
    "RATER = \"Quirin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ab3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb10a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 19\n",
    "prob_range = (0.25, 0.45)\n",
    "n = 100\n",
    "seed = 42\n",
    "\n",
    "\n",
    "lower_prob, upper_prob = prob_range\n",
    "prob_col = f\"topic_{topic_id}\"\n",
    "candidates = df_topics[(df_topics[prob_col] >= lower_prob) & (df_topics[prob_col] <= upper_prob)]\n",
    "\n",
    "sampled = candidates.sample(n=min(n, len(candidates)), random_state=seed).reset_index(drop=True)\n",
    "ratings = []\n",
    "\n",
    "output_path = f\"../../data/lda/manual_ratings_topic{topic_id}_{RATER}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd25b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: speech with max topic score\n",
      "\t I voted in favour of this report. Europe has been paralysed with regard to the migrant crisis. While we keep discussing possible solutions to the grave problem we face, it hardly ever comes to common action. However, even when we do decide on joint measures, we do not follow through on them. The Dublin Agreement and the relocation system have failed utterly, as the Member States have not committed to apply them. We must either effectively enforce the current measures, or introduce new ones. Either way, it is not a problem that can be solved by any Member State alone. It is a dilemma that must be dealt with by the European Union as a whole. It cannot be fixed by closing national borders, but only by guarding our common ones; those defined by Schengen. What we need to do is form a common border guard and coast guard and put a collective reception and relocation system in place. Furthermore, we must prompt Turkey to crack down on the criminals smuggling refugees to Europe under harsh circumstances and we must speak with African governments about returning economic migrants. However, what is most important is that Member States keep to it.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity check: speech with max topic score\\n\\t\", df_topics[df_topics[prob_col] == (df_topics[prob_col]).max()][\"translatedText\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0fdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(prompt= \"Rate (y, n, q):\"): \n",
    "    answer = input(prompt)\n",
    "    if answer == 'q':\n",
    "        return -1\n",
    "    elif answer == 'y': \n",
    "        return 1\n",
    "    elif answer == 'n': \n",
    "        return 0\n",
    "    else: \n",
    "        return process_response(\"Try again (y, n, q):\")\n",
    "\n",
    "def get_rating(sampled, index, prob_col):\n",
    "    row = sampled.iloc[index]\n",
    "\n",
    "    print(f\"Progress: {index}/{len(sampled)} speeches rated\")\n",
    "    print(\"\\n\")\n",
    "    # print(f\"\\nSpeech {index}/{len(sampled)} | Year: {row['year']} | Party block: {row['block']}\")\n",
    "    print(f\"Topic probability: {row[prob_col]:.4f}\")\n",
    "    print(textwrap.fill(row[\"translatedText\"], width=90))\n",
    "\n",
    "    return process_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c4e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual rating: 0/100 speeches to rate.\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74756aee30bc416b9ebfa435cbc19475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "running = True\n",
    "if len(ratings) == len(sampled): \n",
    "    print(\"Ratings are done.\")\n",
    "else: \n",
    "    print(f\"Starting manual rating: {len(ratings)}/{len(sampled)} speeches to rate.\")\n",
    "    print(f\"{'='*90}\\n\")\n",
    "\n",
    "    out = Output()\n",
    "    display(out) \n",
    "    with out: \n",
    "        while running: \n",
    "            clear_output()\n",
    "\n",
    "            current_index = len(ratings)\n",
    "            response = get_rating(sampled, current_index, prob_col)\n",
    "            if response == -1: \n",
    "                clear_output()\n",
    "                print(\"Terminated.\")\n",
    "                print(\"Continue at index\", current_index)\n",
    "                running = False\n",
    "            else: \n",
    "                ratings.append(response)\n",
    "\n",
    "\n",
    "            if current_index >= len(sampled)-1:\n",
    "                print(f\"Rating complete! Rated {len(ratings)}/{len(sampled)} speeches.\")\n",
    "                # Now add ratings to dataframe\n",
    "                sampled.loc[:len(ratings)-1, 'rating'] = ratings\n",
    "                print(f\"\\nFinal ratings: {ratings}\")\n",
    "                running = False\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6b4cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ratings to ../../data/lda/manual_ratings_topic19_Quirin.json\n"
     ]
    }
   ],
   "source": [
    "# save as json\n",
    "ratings = sampled['rating'].tolist()\n",
    "probabilities = sampled[prob_col].tolist()\n",
    "\n",
    "ratings_dict = {\n",
    "    \"ratings\": ratings,\n",
    "    \"probabilities\": probabilities\n",
    "}\n",
    "json.dump(ratings_dict, open(output_path, \"w\"))\n",
    "print(f\"Saved ratings to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1941b61",
   "metadata": {},
   "source": [
    "## Merge ratings from two authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATER_1 = \"Quirin\"\n",
    "RATER_2 = \"Jakob\"\n",
    "path_ratings1 = f\"../../data/lda/manual_ratings_topic{topic_id}_{RATER_1}.json\"\n",
    "path_ratings2 = f\"../../data/lda/manual_ratings_topic{topic_id}_{RATER_2}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4065979",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = json.load(open(path_ratings1, \"r\"))\n",
    "r2 = json.load(open(path_ratings2, \"r\"))\n",
    "\n",
    "ratings1 = np.ndarray(r1[\"ratings\"])\n",
    "ratings2 = np.ndarray(r2[\"ratings\"])\n",
    "\n",
    "probabilities1 = r1[\"probabilities\"]\n",
    "probabilities2 = r2[\"probabilities\"]\n",
    "assert probabilities1 == probabilities2, \"Probabilities do not match between raters!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disagreements = np.array([np.abs(r1 - r2) for r1, r2 in zip(ratings1, ratings2)])\n",
    "new_ratings = dict()\n",
    "\n",
    "for sample_index, r in sampled[disagreements == 1].iterrows():\n",
    "    print(sample_index, r[prob_col])\n",
    "    print(\"Rater 1:\", ratings1[sample_index])\n",
    "    print(\"Rater 2:\", ratings2[sample_index])\n",
    "\n",
    "    print(textwrap.fill(r[\"translatedText\"], width=90))\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    response = process_response(\"Rate again. q to quit, y for keep, n for discard\")\n",
    "    if response == -1: \n",
    "        break \n",
    "    new_ratings[sample_index] = response\n",
    "    clear_output()\n",
    "\n",
    "ratings_merged = ratings1.copy()\n",
    "for i, new_rating in new_ratings.items():\n",
    "    ratings_merged[int(i)] = new_rating\n",
    "\n",
    "# save ratings merged to json\n",
    "ratings_dict[\"ratings_merged\"] = ratings_merged\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(ratings_dict, f)\n",
    "print(f\"Saved merged ratings to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
