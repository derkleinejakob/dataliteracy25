{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import logging\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c818a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[str], custom_stopwords=[], test_first_k = None):     \n",
    "    logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level = logging.WARN)\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    def preprocess_document(document):    \n",
    "        # tokenize using gensim's default preprocessing\n",
    "        tokens = simple_preprocess(document)\n",
    "        document = nlp(\" \".join(tokens))\n",
    "        # lemmatize and remove stopwords \n",
    "        lemmas = [token.lemma_ for token in document if (not token.is_stop) and (not token.lemma_ in custom_stopwords)]\n",
    "        return lemmas\n",
    "\n",
    "    if test_first_k: \n",
    "        documents = documents[:test_first_k]\n",
    "    \n",
    "    processed_data = [preprocess_document(doc) for doc in tqdm(documents, \"preprocessing\")]\n",
    "    return processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d13eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary, search_term = \"migration\", compute_coherence=True): \n",
    "    \"\"\"For the LDA model compute: \n",
    "    - coherence (a metric in LDA to express whether each word is associated with one topic (desireable, coherence => 1) or many (undesireable, coherence => 0)\n",
    "    - highest probability that the search term is given in a topic\n",
    "    - the most frequent position of the search term within the topics (e.g. if search term is most likely word in topic X, its most frequent position will be 0)\n",
    "    - the indices of topics where the search term is within the k most likely words of that topic\n",
    "    \n",
    "    \"\"\"\n",
    "    if compute_coherence: \n",
    "        print(\"Computing coherence\")\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lda_model, \n",
    "            texts=preprocessed_data, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'  # most common coherence measure\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "    else: \n",
    "        coherence_score = None \n",
    "        \n",
    "    \n",
    "    # for each topic get probability of migration \n",
    "    # print k most likely words for 3 topics with highest probability \n",
    "    # return max probability and whether migration appeared in k most likely words of any topic \n",
    "\n",
    "    # find maximum probability of search term in the topics\n",
    "    search_term_max_prob = float(\"-inf\")\n",
    "    search_term_highest_pos = float(\"inf\")\n",
    "    indices_relevant_topics = []\n",
    "    \n",
    "    for topic_index, topic in lda_model.show_topics(formatted=False, num_topics=n_topics):\n",
    "        topic_words, topic_probs = zip(*topic)\n",
    "    \n",
    "        if search_term in topic_words: \n",
    "            idx = topic_words.index(search_term)\n",
    "            search_term_max_prob = max(topic_probs[idx], search_term_max_prob)\n",
    "            search_term_highest_pos = min(idx, search_term_highest_pos)        \n",
    "            # check if search term appears in k most likely words (are ordered by their likelihood)\n",
    "            if idx < k_words: \n",
    "                indices_relevant_topics.append(topic_index)  \n",
    "                label = \", \".join([f\"{word} ({'.2f' % prob})\" for word, prob in topic[:k_words]])\n",
    "                print(f\"Possibly relevant topic {idx + 1}: {label}\")\n",
    "                \n",
    "    print(\".\"*30)\n",
    "    print(\"Coherence:\", coherence_score)\n",
    "    print(f\"Highest probability of {search_term}: {search_term_max_prob}\")\n",
    "    print(f\"Most likely position of {search_term}: {search_term_highest_pos}\")\n",
    "    print(f\"Relevant topics: {indices_relevant_topics} (n: {len(indices_relevant_topics)})\")\n",
    "    return coherence_score, search_term_max_prob, search_term_highest_pos, indices_relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b909d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_path = \"data/parllaw/speech_translated.csv\"\n",
    "df = pd.read_csv(df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use speeches relevant for our project (by party members and with sufficient length)\n",
    "df_party_members = df[~(df[\"party\"] == \"-\")]\n",
    "df_party_members = df_party_members[df_party_members[\"translatedText\"].map(str).map(len) > 50]\n",
    "df_party_members.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf83c36",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Because most of the data was already processed previously (before the missing translations were created), we here only preprpocess the speeches whose translations we made using Gemini. Then we merge the previously preprocessed data back with the appended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data once \n",
    "preprocessed_full_path = \"data/lda/preprocessed_texts_all_translated.json\"\n",
    "if os.path.exists(preprocessed_full_path): \n",
    "    print(\"Loading preprocessed data\")\n",
    "    preprocessed_data = json.load(open(preprocessed_full_path))\n",
    "else: \n",
    "    # TODO: acutally this is not needed anymore, streamline it\n",
    "    print(\"COULD NOT FIND PREPROCESSED DATA; ASSUMING IT HAS TO BE MERGED WITH PREVIOUSLY PREPROCESSED DATA FIRST\")\n",
    "    preprocessed_gemini_path = \"data/lda/preprocessed_texts_gemini_translated.json\"\n",
    "    preprocessed_parllaw_path = \"data/lda/preprocessed_texts_parllaw_translated.json\"\n",
    "\n",
    "    if os.path.exists(preprocessed_gemini_path):\n",
    "        preprocessed_gemini_translated = json.load(open(preprocessed_gemini_path))\n",
    "    else:\n",
    "        # for now: only those translated by gemini: \n",
    "        df_gemini_translated = df_party_members[df_party_members[\"translationSource\"].isin([\"original_gm\", \"machine_gm\"])]\n",
    "        print(\"Number of documents to preprocess:\", len(df_gemini_translated))\n",
    "        \n",
    "        documents = df_gemini_translated[\"translatedText\"].tolist()\n",
    "        preprocessed_gemini_translated = preprocess_documents(documents)\n",
    "        json.dump(preprocessed_gemini_translated, open(preprocessed_gemini_path, \"w+\"))\n",
    "\n",
    "    # merge preprocessed data \n",
    "    preprocessed_parllaw_translated = json.load(open(preprocessed_parllaw_path)) \n",
    "\n",
    "    parllaw_translated_indices = df_party_members[df_party_members[\"translationSource\"].isin([\"original_pl\", \"machine_pl\"])].index.tolist()\n",
    "    gemini_translated_indices = df_gemini_translated.index.tolist()\n",
    "    all_indices = parllaw_translated_indices + gemini_translated_indices\n",
    "    # sanity checks:\n",
    "    assert len(parllaw_translated_indices) == len(preprocessed_parllaw_translated)\n",
    "    assert len(gemini_translated_indices) == len(preprocessed_gemini_translated)\n",
    "    # first just append, but to keep indices aligned with the dataframe's indices, we re-order based on the dataframe's indices\n",
    "    preprocessed_data_unordered = preprocessed_parllaw_translated + preprocessed_gemini_translated\n",
    "    preprocessed_data = [None] * len(preprocessed_data_unordered)\n",
    "    for current_index, target_index in enumerate(all_indices): \n",
    "        preprocessed_data[target_index] = preprocessed_data_unordered[current_index]\n",
    "        \n",
    "    json.dump(preprocessed_data, open(preprocessed_full_path, \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dictionary\")\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "print(\"Filtering dictionary\")\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,     # Keep tokens appearing in at least 10 speeches\n",
    "    no_above=0.4,    # Remove tokens appearing in more than 40% of speeches\n",
    "    keep_n=100000    # Keep only the top 100k words by frequency\n",
    ")\n",
    "corpus = [dictionary.doc2bow(l) for l in tqdm(preprocessed_data, \"Preparing corpus\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed645328",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topic_values = {    \n",
    "    50: [5], \n",
    "    60: [5], \n",
    "    80: [5, 7, 10], \n",
    "    100: [5, 7, 10], \n",
    "    120: [5, 7, 10],\n",
    "}\n",
    "\n",
    "n_workers = 4\n",
    "k_words = 10\n",
    "\n",
    "runs = []\n",
    "for n_topics, n_passes_values in n_topic_values.items(): \n",
    "    for n_passes in n_passes_values: \n",
    "        os.makedirs(f\"data/lda/{n_topics}_topics/{n_passes}\", exist_ok=True)\n",
    "        out_path = f\"data/lda/{n_topics}_topics/{n_passes}/model.model\"\n",
    "        num_topics = n_topics\n",
    "        n_passes = 5\n",
    "        workers = n_workers\n",
    "\n",
    "        print(\"Fitting model with\", num_topics, \"topics and\", n_passes, \"passes\")\n",
    "        lda_model = LdaMulticore(corpus = corpus, id2word=dictionary, num_topics = num_topics, passes = n_passes, workers=workers)\n",
    "        lda_model.save(out_path)\n",
    "        \n",
    "        runs.append((n_topics, n_passes, lda_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7969888",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_topics, n_passes, lda_model in runs: \n",
    "    print(\"Evaluating model with\", n_topics, \"topics and\", n_passes, \"n_passes\")\n",
    "    os.makedirs(f\"data/lda/{n_topics}_topics/{n_passes}\", exist_ok=True)\n",
    "    out_path = f\"data/lda/{n_topics}_topics/{n_passes}/model.model\"\n",
    "    evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
