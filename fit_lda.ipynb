{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c445b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import logging\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c818a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[str], custom_stopwords=[], test_first_k = None):     \n",
    "    logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level = logging.WARN)\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    def preprocess_document(document):    \n",
    "        # tokenize using gensim's default preprocessing\n",
    "        tokens = simple_preprocess(document)\n",
    "        document = nlp(\" \".join(tokens))\n",
    "        # lemmatize and remove stopwords \n",
    "        lemmas = [token.lemma_ for token in document if (not token.is_stop) and (not token.lemma_ in custom_stopwords)]\n",
    "        return lemmas\n",
    "\n",
    "    if test_first_k: \n",
    "        documents = documents[:test_first_k]\n",
    "    \n",
    "    processed_data = [preprocess_document(doc) for doc in tqdm(documents, \"preprocessing\")]\n",
    "    return processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d13eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary, search_term = \"migration\", compute_coherence=True): \n",
    "    \"\"\"For the LDA model compute: \n",
    "    - coherence (a metric in LDA to express whether each word is associated with one topic (desireable, coherence => 1) or many (undesireable, coherence => 0)\n",
    "    - highest probability that the search term is given in a topic\n",
    "    - the most frequent position of the search term within the topics (e.g. if search term is most likely word in topic X, its most frequent position will be 0)\n",
    "    - the indices of topics where the search term is within the k most likely words of that topic\n",
    "    \n",
    "    \"\"\"\n",
    "    if compute_coherence: \n",
    "        print(\"Computing coherence\")\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lda_model, \n",
    "            texts=preprocessed_data, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'  # most common coherence measure\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "    else: \n",
    "        coherence_score = None \n",
    "        \n",
    "    \n",
    "    # for each topic get probability of migration \n",
    "    # print k most likely words for 3 topics with highest probability \n",
    "    # return max probability and whether migration appeared in k most likely words of any topic \n",
    "\n",
    "    # find maximum probability of search term in the topics\n",
    "    search_term_max_prob = float(\"-inf\")\n",
    "    search_term_highest_pos = float(\"inf\")\n",
    "    indices_relevant_topics = []\n",
    "    \n",
    "    for topic_index, topic in lda_model.show_topics(formatted=False, num_topics=n_topics):\n",
    "        topic_words, topic_probs = zip(*topic)\n",
    "        label = \", \".join([f\"{word} ({'.2f' % prob})\" for word, prob in topic[:k_words]])\n",
    "        print(f\"Topic {idx + 1}: {label}\")\n",
    "    \n",
    "        if search_term in topic_words: \n",
    "            idx = topic_words.index(search_term)\n",
    "            search_term_max_prob = max(topic_probs[idx], search_term_max_prob)\n",
    "            search_term_highest_pos = min(idx, search_term_highest_pos)        \n",
    "            # check if search term appears in k most likely words (are ordered by their likelihood)\n",
    "            if idx < k_words: \n",
    "                indices_relevant_topics.append(topic_index)  \n",
    "                print(\"< RELEVANT!\")\n",
    "                \n",
    "    print(\".\"*30)\n",
    "    print(\"Coherence:\", coherence_score)\n",
    "    print(f\"Highest probability of {search_term}: {search_term_max_prob}\")\n",
    "    print(f\"Most likely position of {search_term}: {search_term_highest_pos}\")\n",
    "    print(f\"Relevant topics: {indices_relevant_topics} (n: {len(indices_relevant_topics)})\")\n",
    "    return coherence_score, search_term_max_prob, search_term_highest_pos, indices_relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b909d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df_file_path = \u001b[33m\"\u001b[39m\u001b[33mdata/translation/df_translated.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "df_file_path = \"data/parllaw/speech_translated.csv\"\n",
    "df = pd.read_csv(df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use speeches relevant for our project (by party members and with sufficient length)\n",
    "df_party_members = df[~(df[\"party\"] == \"-\")]\n",
    "df_party_members = df_party_members[df_party_members[\"translatedText\"].map(str).map(len) > 50]\n",
    "df_party_members.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf83c36",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Because most of the data was already processed previously (before the missing translations were created), we here only preprpocess the speeches whose translations we made using Gemini. Then we merge the previously preprocessed data back with the appended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data once \n",
    "preprocessed_full_path = \"data/lda/preprocessed_texts_all_translated.json\"\n",
    "if os.path.exists(preprocessed_full_path): \n",
    "    print(\"Loading preprocessed data\")\n",
    "    preprocessed_data = json.load(open(preprocessed_full_path))\n",
    "else: \n",
    "    # TODO: acutally this is not needed anymore, streamline it\n",
    "    print(\"COULD NOT FIND PREPROCESSED DATA; ASSUMING IT HAS TO BE MERGED WITH PREVIOUSLY PREPROCESSED DATA FIRST\")\n",
    "    preprocessed_gemini_path = \"data/lda/preprocessed_texts_gemini_translated.json\"\n",
    "    preprocessed_parllaw_path = \"data/lda/preprocessed_texts_parllaw_translated.json\"\n",
    "\n",
    "    if os.path.exists(preprocessed_gemini_path):\n",
    "        preprocessed_gemini_translated = json.load(open(preprocessed_gemini_path))\n",
    "    else:\n",
    "        # for now: only those translated by gemini: \n",
    "        df_gemini_translated = df_party_members[df_party_members[\"translationSource\"].isin([\"original_gm\", \"machine_gm\"])]\n",
    "        print(\"Number of documents to preprocess:\", len(df_gemini_translated))\n",
    "        \n",
    "        documents = df_gemini_translated[\"translatedText\"].tolist()\n",
    "        preprocessed_gemini_translated = preprocess_documents(documents)\n",
    "        json.dump(preprocessed_gemini_translated, open(preprocessed_gemini_path, \"w+\"))\n",
    "\n",
    "    # merge preprocessed data \n",
    "    preprocessed_parllaw_translated = json.load(open(preprocessed_parllaw_path)) \n",
    "\n",
    "    parllaw_translated_indices = df_party_members[df_party_members[\"translationSource\"].isin([\"original_pl\", \"machine_pl\"])].index.tolist()\n",
    "    gemini_translated_indices = df_gemini_translated.index.tolist()\n",
    "    all_indices = parllaw_translated_indices + gemini_translated_indices\n",
    "    # sanity checks:\n",
    "    assert len(parllaw_translated_indices) == len(preprocessed_parllaw_translated)\n",
    "    assert len(gemini_translated_indices) == len(preprocessed_gemini_translated)\n",
    "    # first just append, but to keep indices aligned with the dataframe's indices, we re-order based on the dataframe's indices\n",
    "    preprocessed_data_unordered = preprocessed_parllaw_translated + preprocessed_gemini_translated\n",
    "    preprocessed_data = [None] * len(preprocessed_data_unordered)\n",
    "    for current_index, target_index in enumerate(all_indices): \n",
    "        preprocessed_data[target_index] = preprocessed_data_unordered[current_index]\n",
    "        \n",
    "    json.dump(preprocessed_data, open(preprocessed_full_path, \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82bb40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary\n",
      "Filtering dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing corpus: 100%|██████████| 505705/505705 [00:33<00:00, 15084.70it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating dictionary\")\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "print(\"Filtering dictionary\")\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,     # Keep tokens appearing in at least 10 speeches\n",
    "    no_above=0.4,    # Remove tokens appearing in more than 40% of speeches\n",
    "    keep_n=100000    # Keep only the top 100k words by frequency\n",
    ")\n",
    "corpus = [dictionary.doc2bow(l) for l in tqdm(preprocessed_data, \"Preparing corpus\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed645328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 50 topics and 5 passes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Process SpawnPoolWorker-3:\n",
      "Process SpawnPoolWorker-4:\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/pool.py\", line 109, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py\", line 346, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 769, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 720, in inference\n",
      "    gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/pool.py\", line 109, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py\", line 344, in worker_e_step\n",
      "    worker_lda.sync_state()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 635, in sync_state\n",
      "    current_Elogbeta = self.state.get_Elogbeta()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 283, in get_Elogbeta\n",
      "    return dirichlet_expectation(self.get_lambda())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/pool.py\", line 109, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py\", line 344, in worker_e_step\n",
      "    worker_lda.sync_state()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 635, in sync_state\n",
      "    current_Elogbeta = self.state.get_Elogbeta()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 283, in get_Elogbeta\n",
      "    return dirichlet_expectation(self.get_lambda())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/multiprocessing/pool.py\", line 109, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py\", line 346, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 769, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py\", line 734, in inference\n",
      "    sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/einkleinerjakob/miniforge3/envs/b2/lib/python3.11/site-packages/numpy/_core/numeric.py\", line 906, in _outer_dispatcher\n",
      "    def _outer_dispatcher(a, b, out=None):\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFull\u001b[39m                                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py:298\u001b[39m, in \u001b[36mLdaMulticore.update\u001b[39m\u001b[34m(self, corpus, chunks_as_numpy)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[43mjob_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_no\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     queue_size[\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/multiprocessing/queues.py:90\u001b[39m, in \u001b[36mQueue.put\u001b[39m\u001b[34m(self, obj, block, timeout)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sem.acquire(block, timeout):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Full\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._notempty:\n",
      "\u001b[31mFull\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m workers = n_workers\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting model with\u001b[39m\u001b[33m\"\u001b[39m, num_topics, \u001b[33m\"\u001b[39m\u001b[33mtopics and\u001b[39m\u001b[33m\"\u001b[39m, n_passes, \u001b[33m\"\u001b[39m\u001b[33mpasses\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m lda_model = \u001b[43mLdaMulticore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_passes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m lda_model.save(out_path)\n\u001b[32m     18\u001b[39m evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py:186\u001b[39m, in \u001b[36mLdaMulticore.__init__\u001b[39m\u001b[34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alpha, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m alpha == \u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mauto-tuning alpha not implemented in LdaMulticore; use plain LdaModel.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLdaMulticore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimum_probability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mminimum_probability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminimum_phi_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mminimum_phi_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_word_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mper_word_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py:522\u001b[39m, in \u001b[36mLdaModel.__init__\u001b[39m\u001b[34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[39m\n\u001b[32m    520\u001b[39m use_numpy = \u001b[38;5;28mself\u001b[39m.dispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    521\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\n\u001b[32m    524\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    525\u001b[39m     msg=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    526\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py:309\u001b[39m, in \u001b[36mLdaMulticore.update\u001b[39m\u001b[34m(self, corpus, chunks_as_numpy)\u001b[39m\n\u001b[32m    305\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    306\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m queue.Full:\n\u001b[32m    307\u001b[39m             \u001b[38;5;66;03m# in case the input job queue is full, keep clearing the\u001b[39;00m\n\u001b[32m    308\u001b[39m             \u001b[38;5;66;03m# result queue, to make sure we don't deadlock\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m             \u001b[43mprocess_result_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m     process_result_queue()\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# endfor single corpus pass\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# wait for all outstanding jobs to finish\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamulticore.py:280\u001b[39m, in \u001b[36mLdaMulticore.update.<locals>.process_result_queue\u001b[39m\u001b[34m(force)\u001b[39m\n\u001b[32m    277\u001b[39m     merged_new = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (force \u001b[38;5;129;01mand\u001b[39;00m merged_new \u001b[38;5;129;01mand\u001b[39;00m queue_size[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (other.numdocs >= updateafter):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     other.reset()\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_every > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (force \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.num_updates / updateafter) % eval_every == \u001b[32m0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py:1068\u001b[39m, in \u001b[36mLdaModel.do_mstep\u001b[39m\u001b[34m(self, rho, other, extra_pass)\u001b[39m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# update self with the new blend; also keep track of how much did\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# the topics change through this update, to assess convergence\u001b[39;00m\n\u001b[32m   1067\u001b[39m previous_Elogbeta = \u001b[38;5;28mself\u001b[39m.state.get_Elogbeta()\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m current_Elogbeta = \u001b[38;5;28mself\u001b[39m.state.get_Elogbeta()\n\u001b[32m   1071\u001b[39m \u001b[38;5;28mself\u001b[39m.sync_state(current_Elogbeta)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/b2/lib/python3.11/site-packages/gensim/models/ldamodel.py:201\u001b[39m, in \u001b[36mLdaState.blend\u001b[39m\u001b[34m(self, rhot, other, targetsize)\u001b[39m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m.sstats += other.sstats\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.numdocs += other.numdocs\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mblend\u001b[39m(\u001b[38;5;28mself\u001b[39m, rhot, other, targetsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    202\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Merge the current state with another one using a weighted average for the sufficient statistics.\u001b[39;00m\n\u001b[32m    203\u001b[39m \n\u001b[32m    204\u001b[39m \u001b[33;03m    The number of documents is stretched in both state objects, so that they are of comparable magnitude.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m other \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_topic_values = {    \n",
    "    50: [5], \n",
    "    60: [5], \n",
    "    80: [5, 7, 10], \n",
    "    100: [5, 7, 10], \n",
    "    120: [5, 7, 10],\n",
    "}\n",
    "\n",
    "n_workers = 4\n",
    "k_words = 10\n",
    "\n",
    "for n_topics, n_passes_values in n_topic_values: \n",
    "    for n_passes in n_passes_values: \n",
    "        os.makedirs(f\"data/lda/{n_topics}_topics/{n_passes}\", exist_ok=True)\n",
    "        out_path = f\"data/lda/{n_topics}_topics/{n_passes}/model.model\"\n",
    "        num_topics = n_topics\n",
    "        n_passes = 5\n",
    "        workers = n_workers\n",
    "\n",
    "        print(\"Fitting model with\", num_topics, \"topics and\", n_passes, \"passes\")\n",
    "        lda_model = LdaMulticore(corpus = corpus, id2word=dictionary, num_topics = num_topics, passes = n_passes, workers=workers)\n",
    "        lda_model.save(out_path)\n",
    "\n",
    "        evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7969888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
