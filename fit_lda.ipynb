{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c445b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import logging\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c818a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[str], custom_stopwords=[], test_first_k = None): \n",
    "    logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level = logging.WARN)\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    def preprocess_document(document):    \n",
    "        # tokenize using gensim's default preprocessing\n",
    "        tokens = simple_preprocess(document)\n",
    "        document = nlp(\" \".join(tokens))\n",
    "        # lemmatize and remove stopwords \n",
    "        lemmas = [token.lemma_ for token in document if (not token.is_stop) and (not token.lemma_ in custom_stopwords)]\n",
    "        return lemmas\n",
    "\n",
    "    if test_first_k: \n",
    "        documents = documents[:test_first_k]\n",
    "    \n",
    "    processed_data = [preprocess_document(doc) for doc in tqdm(documents, \"preprocessing\")]\n",
    "    return processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b909d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_path = \"data/csv/df_translated.csv\"\n",
    "df = pd.read_csv(df_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc62838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'machine_gm', 'orginial_gm', 'original_pl', 'machine_pl'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"translationSource\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3f1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"translationSource\"] == \"orginial_gm\", \"translationSource\"] = \"original_gm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fc1793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'machine_gm', 'original_gm', 'original_pl', 'machine_pl'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"translationSource\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d55e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_party_members = df[~(df[\"party\"] == \"-\")]\n",
    "df_party_members = df_party_members[df_party_members[\"translatedText\"].map(str).map(len) > 50]\n",
    "df_party_members.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3cb264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to preprocess: 52293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 100%|██████████| 52293/52293 [11:08<00:00, 78.23it/s] \n"
     ]
    }
   ],
   "source": [
    "# preprocess data once \n",
    "preprocessed_path = \"data/lda/preprocessed_texts_gemini_translated.json\"\n",
    "\n",
    "if os.path.exists(preprocessed_path):\n",
    "    preprocessed_gemini_translated = json.load(open(preprocessed_path))\n",
    "else:\n",
    "    # for now: only those translated by gemini: \n",
    "    # only those with party affiliation\n",
    "    df_gemini_translated = df_party_members[df_party_members[\"translationSource\"].isin([\"original_gm\", \"machine_gm\"])]\n",
    "    print(\"Number of documents to preprocess:\", len(df_gemini_translated))\n",
    "    \n",
    "    documents = df_gemini_translated[\"translatedText\"].tolist()\n",
    "    preprocessed_gemini_translated = preprocess_documents(documents)\n",
    "    json.dump(preprocessed_gemini_translated, open(preprocessed_path, \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4373443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge preprocessed data \n",
    "preprocessed_parllaw_translated = json.load(open(\"data/lda/preprocessed_texts_parllaw_translated.json\")) \n",
    "\n",
    "parllaw_translated_indices = df_party_members[df_party_members[\"translationSource\"].isin([\"original_pl\", \"machine_pl\"])].index.tolist()\n",
    "gemini_translated_indices = df_gemini_translated.index.tolist()\n",
    "all_indices = parllaw_translated_indices + gemini_translated_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19f0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['machine_gm', 'orginial_gm', 'original_pl', 'machine_pl'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0d023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453412 + 52293\n",
      "505705\n"
     ]
    }
   ],
   "source": [
    "assert len(parllaw_translated_indices) == len(preprocessed_parllaw_translated)\n",
    "assert len(gemini_translated_indices) == len(preprocessed_gemini_translated)\n",
    "\n",
    "print(len(preprocessed_parllaw_translated), \"+\", len(preprocessed_gemini_translated))\n",
    "print(len(df_party_members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4347cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_unordered = preprocessed_parllaw_translated + preprocessed_gemini_translated\n",
    "preprocessed_data = [None] * len(preprocessed_data_unordered)\n",
    "for current_index, target_index in enumerate(all_indices): \n",
    "    preprocessed_data[target_index] = preprocessed_data_unordered[current_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d4cf321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505705\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "528423b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(preprocessed_data, open(\"data/lda/preprocessed_texts_all_translated.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82bb40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "filtering dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing corpus: 100%|██████████| 505705/505705 [00:28<00:00, 17995.23it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"creating dictionary\")\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "print(\"filtering dictionary\")\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,     # Keep tokens appearing in at least 10 docs\n",
    "    no_above=0.4,    # Remove tokens appearing in more than 40% of docs\n",
    "    keep_n=100000    # Keep only the top 100k words by frequency\n",
    ")\n",
    "corpus = [dictionary.doc2bow(l) for l in tqdm(preprocessed_data, \"Preparing corpus\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed645328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 80 topics and 5 passes\n"
     ]
    }
   ],
   "source": [
    "n_topic_values = [80]\n",
    "n_workers = 8\n",
    "\n",
    "for n_topics in n_topic_values: \n",
    "    os.makedirs(f\"lda/{n_topics}_topics\", exist_ok=True)\n",
    "    out_path = f\"lda/{n_topics}_topics/model.model\"\n",
    "    num_topics = n_topics\n",
    "    n_passes = 5\n",
    "    workers = n_workers\n",
    "\n",
    "    print(\"Fitting model with\", num_topics, \"topics and\", n_passes, \"passes\")\n",
    "    lda_model = LdaMulticore(corpus = corpus, id2word=dictionary, num_topics = num_topics, passes = n_passes, workers=workers)\n",
    "    lda_model.save(out_path)\n",
    "\n",
    "    # Evaluate model\n",
    "    # evaluate_model(lda_model, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15361c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29123b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
