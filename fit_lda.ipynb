{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c445b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import logging\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c818a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[str], custom_stopwords=[], test_first_k = None):     \n",
    "    logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level = logging.WARN)\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    def preprocess_document(document):    \n",
    "        # tokenize using gensim's default preprocessing\n",
    "        tokens = simple_preprocess(document)\n",
    "        document = nlp(\" \".join(tokens))\n",
    "        # lemmatize and remove stopwords \n",
    "        lemmas = [token.lemma_ for token in document if (not token.is_stop) and (not token.lemma_ in custom_stopwords)]\n",
    "        return lemmas\n",
    "\n",
    "    if test_first_k: \n",
    "        documents = documents[:test_first_k]\n",
    "    \n",
    "    processed_data = [preprocess_document(doc) for doc in tqdm(documents, \"preprocessing\")]\n",
    "    return processed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d13eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary, search_term = \"migration\", compute_coherence=True): \n",
    "    \"\"\"For the LDA model compute: \n",
    "    - coherence (a metric in LDA to express whether each word is associated with one topic (desireable, coherence => 1) or many (undesireable, coherence => 0)\n",
    "    - highest probability that the search term is given in a topic\n",
    "    - the most frequent position of the search term within the topics (e.g. if search term is most likely word in topic X, its most frequent position will be 0)\n",
    "    - the indices of topics where the search term is within the k most likely words of that topic\n",
    "    \n",
    "    \"\"\"\n",
    "    if compute_coherence: \n",
    "        print(\"Computing coherence\")\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lda_model, \n",
    "            texts=preprocessed_data, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'  # most common coherence measure\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "    else: \n",
    "        coherence_score = None \n",
    "        \n",
    "    \n",
    "    # for each topic get probability of migration \n",
    "    # print k most likely words for 3 topics with highest probability \n",
    "    # return max probability and whether migration appeared in k most likely words of any topic \n",
    "\n",
    "    # find maximum probability of search term in the topics\n",
    "    search_term_max_prob = float(\"-inf\")\n",
    "    search_term_highest_pos = float(\"inf\")\n",
    "    indices_relevant_topics = []\n",
    "    \n",
    "    for topic_index, topic in lda_model.show_topics(formatted=False, num_topics=n_topics):\n",
    "        topic_words, topic_probs = zip(*topic)\n",
    "    \n",
    "        if search_term in topic_words: \n",
    "            idx = topic_words.index(search_term)\n",
    "            search_term_max_prob = max(topic_probs[idx], search_term_max_prob)\n",
    "            search_term_highest_pos = min(idx, search_term_highest_pos)        \n",
    "            # check if search term appears in k most likely words (are ordered by their likelihood)\n",
    "            if idx < k_words: \n",
    "                indices_relevant_topics.append(topic_index)  \n",
    "\n",
    "                label = \", \".join([f\"{word} ({'%.2f' % prob})\" for word, prob in topic[:k_words]])\n",
    "                print(f\"Possibly relevant topic {idx + 1}: {label}\")\n",
    "                \n",
    "    print(\".\"*30)\n",
    "    print(\"Coherence:\", coherence_score)\n",
    "    print(f\"Highest probability of {search_term}: {search_term_max_prob}\")\n",
    "    print(f\"Most likely position of {search_term}: {search_term_highest_pos}\")\n",
    "    print(f\"Relevant topics: {indices_relevant_topics} (n: {len(indices_relevant_topics)})\")\n",
    "    return coherence_score, search_term_max_prob, search_term_highest_pos, indices_relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b909d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_path = \"data/parllaw/speech_translated.csv\"\n",
    "df = pd.read_csv(df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d55e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use speeches relevant for our project (by party members and with sufficient length)\n",
    "df_party_members = df[~(df[\"party\"] == \"-\")]\n",
    "df_party_members = df_party_members[df_party_members[\"translatedText\"].map(str).map(len) > 50]\n",
    "df_party_members.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop duplicates: for now, only keep the first occurrence of each speech\n",
    "df_party_members = df_party_members.sort_values(by=[\"date\"]).drop_duplicates(subset=[\"text\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e1f78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
       "            9,\n",
       "       ...\n",
       "       505695, 505696, 505697, 505698, 505699, 505700, 505701, 505702, 505703,\n",
       "       505704],\n",
       "      dtype='int64', length=487781)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_party_members.sort_index().index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf83c36",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Because most of the data was already processed previously (before the missing translations were created), we here only preprpocess the speeches whose translations we made using Gemini. Then we merge the previously preprocessed data back with the appended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3cb264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data\n"
     ]
    }
   ],
   "source": [
    "# preprocess data once \n",
    "preprocessed_full_path = \"data/lda/preprocessed_texts_all_translated.json\"\n",
    "if os.path.exists(preprocessed_full_path): \n",
    "    print(\"Loading preprocessed data\")\n",
    "    preprocessed_data = json.load(open(preprocessed_full_path))\n",
    "    preprocessed_data = [preprocessed_data[i] for i in df_party_members.index.tolist()]  # align with filtered dataframe\n",
    "else: \n",
    "    # TODO: acutally this is not needed anymore, streamline it\n",
    "    print(\"COULD NOT FIND PREPROCESSED DATA; ASSUMING IT HAS TO BE MERGED WITH PREVIOUSLY PREPROCESSED DATA FIRST\")\n",
    "    preprocessed_gemini_path = \"data/lda/preprocessed_texts_gemini_translated.json\"\n",
    "    preprocessed_parllaw_path = \"data/lda/preprocessed_texts_parllaw_translated.json\"\n",
    "\n",
    "    if os.path.exists(preprocessed_gemini_path):\n",
    "        preprocessed_gemini_translated = json.load(open(preprocessed_gemini_path))\n",
    "    else:\n",
    "        # for now: only those translated by gemini: \n",
    "        df_gemini_translated = df_party_members[df_party_members[\"translationSource\"].isin([\"original_gm\", \"machine_gm\"])]\n",
    "        print(\"Number of documents to preprocess:\", len(df_gemini_translated))\n",
    "        \n",
    "        documents = df_gemini_translated[\"translatedText\"].tolist()\n",
    "        preprocessed_gemini_translated = preprocess_documents(documents)\n",
    "        json.dump(preprocessed_gemini_translated, open(preprocessed_gemini_path, \"w+\"))\n",
    "\n",
    "    # merge preprocessed data \n",
    "    preprocessed_parllaw_translated = json.load(open(preprocessed_parllaw_path)) \n",
    "\n",
    "    parllaw_translated_indices = df_party_members[df_party_members[\"translationSource\"].isin([\"original_pl\", \"machine_pl\"])].index.tolist()\n",
    "    gemini_translated_indices = df_gemini_translated.index.tolist()\n",
    "    all_indices = parllaw_translated_indices + gemini_translated_indices\n",
    "    # sanity checks:\n",
    "    assert len(parllaw_translated_indices) == len(preprocessed_parllaw_translated)\n",
    "    assert len(gemini_translated_indices) == len(preprocessed_gemini_translated)\n",
    "    # first just append, but to keep indices aligned with the dataframe's indices, we re-order based on the dataframe's indices\n",
    "    preprocessed_data_unordered = preprocessed_parllaw_translated + preprocessed_gemini_translated\n",
    "    preprocessed_data = [None] * len(preprocessed_data_unordered)\n",
    "    for current_index, target_index in enumerate(all_indices): \n",
    "        preprocessed_data[target_index] = preprocessed_data_unordered[current_index]\n",
    "        \n",
    "    json.dump(preprocessed_data, open(preprocessed_full_path, \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82bb40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary\n",
      "Filtering dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing corpus: 100%|██████████| 487781/487781 [00:30<00:00, 15754.36it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating dictionary\")\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "print(\"Filtering dictionary\")\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,     # Keep tokens appearing in at least 10 speeches\n",
    "    no_above=0.4,    # Remove tokens appearing in more than 40% of speeches\n",
    "    keep_n=100000    # Keep only the top 100k words by frequency\n",
    ")\n",
    "corpus = [dictionary.doc2bow(l) for l in tqdm(preprocessed_data, \"Preparing corpus\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed645328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 50 topics and 5 passes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 50 topics and 7 passes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 50 topics and 10 passes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: overflow encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/models/ldamodel.py:720: RuntimeWarning: invalid value encountered in dot\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 60 topics and 5 passes\n",
      "Fitting model with 80 topics and 5 passes\n",
      "Fitting model with 100 topics and 5 passes\n",
      "Fitting model with 120 topics and 5 passes\n"
     ]
    }
   ],
   "source": [
    "n_topic_values = {    \n",
    "    50: [5, 7, 10], \n",
    "    60: [5], \n",
    "    80: [5], \n",
    "    100: [5], \n",
    "    120: [5],\n",
    "}\n",
    "\n",
    "n_workers = 4\n",
    "k_words = 10\n",
    "\n",
    "runs = []\n",
    "for n_topics, n_passes_values in n_topic_values.items(): \n",
    "    for n_passes in n_passes_values: \n",
    "        os.makedirs(f\"data/lda/{n_topics}_topics/{n_passes}\", exist_ok=True)\n",
    "        out_path = f\"data/lda/{n_topics}_topics/{n_passes}/model.model\"\n",
    "        workers = n_workers\n",
    "\n",
    "        print(\"Fitting model with\", n_topics, \"topics and\", n_passes, \"passes\")\n",
    "        lda_model = LdaMulticore(corpus = corpus, id2word=dictionary, num_topics = n_topics, passes = n_passes, workers=workers)\n",
    "        lda_model.save(out_path)\n",
    "        \n",
    "        runs.append((n_topics, n_passes, lda_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7969888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Evaluating model with 50 topics and 5 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 2: border (.2f), migration (.2f), refugee (.2f), country (.2f), migrant (.2f), asylum (.2f), state (.2f), member (.2f), union (.2f), immigration (.2f)\n",
      "..............................\n",
      "Coherence: 0.5105722823466702\n",
      "Highest probability of migration: 0.035696715116500854\n",
      "Most likely position of migration: 1\n",
      "Relevant topics: [40] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 50 topics and 7 n_passes\n",
      "Computing coherence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "/Users/Quirin/anaconda3/envs/dlit/lib/python3.12/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possibly relevant topic 2: border (.2f), migration (.2f), refugee (.2f), migrant (.2f), asylum (.2f), country (.2f), illegal (.2f), europe (.2f), immigration (.2f), people (.2f)\n",
      "..............................\n",
      "Coherence: nan\n",
      "Highest probability of migration: 0.03652923181653023\n",
      "Most likely position of migration: 1\n",
      "Relevant topics: [10] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 50 topics and 10 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 1: migration (.2f), refugee (.2f), border (.2f), migrant (.2f), country (.2f), asylum (.2f), illegal (.2f), europe (.2f), immigration (.2f), people (.2f)\n",
      "..............................\n",
      "Coherence: 0.521065291134682\n",
      "Highest probability of migration: 0.04109171777963638\n",
      "Most likely position of migration: 0\n",
      "Relevant topics: [43] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 60 topics and 5 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 1: migration (.2f), refugee (.2f), border (.2f), migrant (.2f), asylum (.2f), country (.2f), state (.2f), illegal (.2f), immigration (.2f), europe (.2f)\n",
      "..............................\n",
      "Coherence: nan\n",
      "Highest probability of migration: 0.04068572074174881\n",
      "Most likely position of migration: 0\n",
      "Relevant topics: [41] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 80 topics and 5 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 1: migration (.2f), refugee (.2f), migrant (.2f), asylum (.2f), country (.2f), border (.2f), immigration (.2f), illegal (.2f), europe (.2f), union (.2f)\n",
      "..............................\n",
      "Coherence: 0.4972728204522672\n",
      "Highest probability of migration: 0.04566721245646477\n",
      "Most likely position of migration: 0\n",
      "Relevant topics: [63] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 100 topics and 5 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 1: migration (.2f), refugee (.2f), migrant (.2f), asylum (.2f), country (.2f), immigration (.2f), illegal (.2f), state (.2f), policy (.2f), people (.2f)\n",
      "..............................\n",
      "Coherence: 0.4951606490551208\n",
      "Highest probability of migration: 0.0483381487429142\n",
      "Most likely position of migration: 0\n",
      "Relevant topics: [37] (n: 1)\n",
      "\n",
      "\n",
      "\n",
      "Evaluating model with 120 topics and 5 n_passes\n",
      "Computing coherence\n",
      "Possibly relevant topic 5: refugee (.2f), country (.2f), people (.2f), crisis (.2f), migration (.2f), solidarity (.2f), situation (.2f), flee (.2f), humanitarian (.2f), europe (.2f)\n",
      "Possibly relevant topic 1: migration (.2f), illegal (.2f), migrant (.2f), asylum (.2f), immigration (.2f), country (.2f), human (.2f), trafficking (.2f), policy (.2f), immigrant (.2f)\n",
      "..............................\n",
      "Coherence: 0.49094603200298115\n",
      "Highest probability of migration: 0.06386416405439377\n",
      "Most likely position of migration: 0\n",
      "Relevant topics: [14, 91] (n: 2)\n"
     ]
    }
   ],
   "source": [
    "for n_topics, n_passes, lda_model in runs: \n",
    "    print(\"\\n\"*2)\n",
    "    print(\"Evaluating model with\", n_topics, \"topics and\", n_passes, \"n_passes\")\n",
    "    os.makedirs(f\"data/lda/{n_topics}_topics/{n_passes}\", exist_ok=True)\n",
    "    out_path = f\"data/lda/{n_topics}_topics/{n_passes}/model.model\"\n",
    "    evaluate_model(lda_model, n_topics, k_words, preprocessed_data, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
